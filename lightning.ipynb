{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "COPw8ZdZbgXI",
        "outputId": "1436dd11-c08d-40cb-8eee-929e25f68d62"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Found existing installation: tensorflow 2.15.0\n",
            "Uninstalling tensorflow-2.15.0:\n",
            "  Successfully uninstalled tensorflow-2.15.0\n",
            "Collecting tensorflow\n",
            "  Downloading tensorflow-2.15.0.post1-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (475.2 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m475.2/475.2 MB\u001b[0m \u001b[31m3.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: opencv-python in /usr/local/lib/python3.10/dist-packages (4.8.0.76)\n",
            "Requirement already satisfied: matplotlib in /usr/local/lib/python3.10/dist-packages (3.7.1)\n",
            "Requirement already satisfied: absl-py>=1.0.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (1.4.0)\n",
            "Requirement already satisfied: astunparse>=1.6.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (1.6.3)\n",
            "Requirement already satisfied: flatbuffers>=23.5.26 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (23.5.26)\n",
            "Requirement already satisfied: gast!=0.5.0,!=0.5.1,!=0.5.2,>=0.2.1 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (0.5.4)\n",
            "Requirement already satisfied: google-pasta>=0.1.1 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (0.2.0)\n",
            "Requirement already satisfied: h5py>=2.9.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (3.9.0)\n",
            "Requirement already satisfied: libclang>=13.0.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (16.0.6)\n",
            "Requirement already satisfied: ml-dtypes~=0.2.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (0.2.0)\n",
            "Requirement already satisfied: numpy<2.0.0,>=1.23.5 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (1.23.5)\n",
            "Requirement already satisfied: opt-einsum>=2.3.2 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (3.3.0)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.10/dist-packages (from tensorflow) (23.2)\n",
            "Requirement already satisfied: protobuf!=4.21.0,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<5.0.0dev,>=3.20.3 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (3.20.3)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.10/dist-packages (from tensorflow) (67.7.2)\n",
            "Requirement already satisfied: six>=1.12.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (1.16.0)\n",
            "Requirement already satisfied: termcolor>=1.1.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (2.4.0)\n",
            "Requirement already satisfied: typing-extensions>=3.6.6 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (4.9.0)\n",
            "Requirement already satisfied: wrapt<1.15,>=1.11.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (1.14.1)\n",
            "Requirement already satisfied: tensorflow-io-gcs-filesystem>=0.23.1 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (0.35.0)\n",
            "Requirement already satisfied: grpcio<2.0,>=1.24.3 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (1.60.1)\n",
            "Requirement already satisfied: tensorboard<2.16,>=2.15 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (2.15.1)\n",
            "Requirement already satisfied: tensorflow-estimator<2.16,>=2.15.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (2.15.0)\n",
            "Requirement already satisfied: keras<2.16,>=2.15.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (2.15.0)\n",
            "Requirement already satisfied: contourpy>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib) (1.2.0)\n",
            "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.10/dist-packages (from matplotlib) (0.12.1)\n",
            "Requirement already satisfied: fonttools>=4.22.0 in /usr/local/lib/python3.10/dist-packages (from matplotlib) (4.47.2)\n",
            "Requirement already satisfied: kiwisolver>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib) (1.4.5)\n",
            "Requirement already satisfied: pillow>=6.2.0 in /usr/local/lib/python3.10/dist-packages (from matplotlib) (9.4.0)\n",
            "Requirement already satisfied: pyparsing>=2.3.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib) (3.1.1)\n",
            "Requirement already satisfied: python-dateutil>=2.7 in /usr/local/lib/python3.10/dist-packages (from matplotlib) (2.8.2)\n",
            "Requirement already satisfied: wheel<1.0,>=0.23.0 in /usr/local/lib/python3.10/dist-packages (from astunparse>=1.6.0->tensorflow) (0.42.0)\n",
            "Requirement already satisfied: google-auth<3,>=1.6.3 in /usr/local/lib/python3.10/dist-packages (from tensorboard<2.16,>=2.15->tensorflow) (2.17.3)\n",
            "Requirement already satisfied: google-auth-oauthlib<2,>=0.5 in /usr/local/lib/python3.10/dist-packages (from tensorboard<2.16,>=2.15->tensorflow) (1.2.0)\n",
            "Requirement already satisfied: markdown>=2.6.8 in /usr/local/lib/python3.10/dist-packages (from tensorboard<2.16,>=2.15->tensorflow) (3.5.2)\n",
            "Requirement already satisfied: requests<3,>=2.21.0 in /usr/local/lib/python3.10/dist-packages (from tensorboard<2.16,>=2.15->tensorflow) (2.31.0)\n",
            "Requirement already satisfied: tensorboard-data-server<0.8.0,>=0.7.0 in /usr/local/lib/python3.10/dist-packages (from tensorboard<2.16,>=2.15->tensorflow) (0.7.2)\n",
            "Requirement already satisfied: werkzeug>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from tensorboard<2.16,>=2.15->tensorflow) (3.0.1)\n",
            "Requirement already satisfied: cachetools<6.0,>=2.0.0 in /usr/local/lib/python3.10/dist-packages (from google-auth<3,>=1.6.3->tensorboard<2.16,>=2.15->tensorflow) (5.3.2)\n",
            "Requirement already satisfied: pyasn1-modules>=0.2.1 in /usr/local/lib/python3.10/dist-packages (from google-auth<3,>=1.6.3->tensorboard<2.16,>=2.15->tensorflow) (0.3.0)\n",
            "Requirement already satisfied: rsa<5,>=3.1.4 in /usr/local/lib/python3.10/dist-packages (from google-auth<3,>=1.6.3->tensorboard<2.16,>=2.15->tensorflow) (4.9)\n",
            "Requirement already satisfied: requests-oauthlib>=0.7.0 in /usr/local/lib/python3.10/dist-packages (from google-auth-oauthlib<2,>=0.5->tensorboard<2.16,>=2.15->tensorflow) (1.3.1)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.21.0->tensorboard<2.16,>=2.15->tensorflow) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.21.0->tensorboard<2.16,>=2.15->tensorflow) (3.6)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.21.0->tensorboard<2.16,>=2.15->tensorflow) (2.0.7)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.21.0->tensorboard<2.16,>=2.15->tensorflow) (2024.2.2)\n",
            "Requirement already satisfied: MarkupSafe>=2.1.1 in /usr/local/lib/python3.10/dist-packages (from werkzeug>=1.0.1->tensorboard<2.16,>=2.15->tensorflow) (2.1.5)\n",
            "Requirement already satisfied: pyasn1<0.6.0,>=0.4.6 in /usr/local/lib/python3.10/dist-packages (from pyasn1-modules>=0.2.1->google-auth<3,>=1.6.3->tensorboard<2.16,>=2.15->tensorflow) (0.5.1)\n",
            "Requirement already satisfied: oauthlib>=3.0.0 in /usr/local/lib/python3.10/dist-packages (from requests-oauthlib>=0.7.0->google-auth-oauthlib<2,>=0.5->tensorboard<2.16,>=2.15->tensorflow) (3.2.2)\n",
            "Installing collected packages: tensorflow\n",
            "Successfully installed tensorflow-2.15.0.post1\n"
          ]
        }
      ],
      "source": [
        "!pip uninstall -y tensorflow\n",
        "!pip install tensorflow opencv-python matplotlib\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "fdYMVgOTcwVW"
      },
      "outputs": [],
      "source": [
        "import cv2\n",
        "import tensorflow as tf\n",
        "import numpy as np\n",
        "from IPython.display import display, Image\n",
        "import time\n",
        "from google.colab.patches import cv2_imshow\n",
        "import cv2\n",
        "import tensorflow_hub as hub\n",
        "from matplotlib import pyplot as plt\n",
        "import numpy as np\n",
        "from google.colab.patches import cv2_imshow\n",
        "from collections import deque\n",
        "import os\n",
        "import subprocess\n",
        "import shutil"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "BB33R2HwvgWO"
      },
      "outputs": [],
      "source": [
        "model = hub.load('https://tfhub.dev/google/movenet/multipose/lightning/1')\n",
        "movenet = model.signatures['serving_default']"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "FXY_BxX6wPlB"
      },
      "outputs": [],
      "source": [
        "def loop_through_people(frame, keypoints, edges, confidence_threshold):\n",
        "    y, x, c = frame.shape\n",
        "    for person_keypoints in keypoints:\n",
        "        person_keypoints = person_keypoints.reshape((17, 3))  # Reshape to the correct format\n",
        "\n",
        "        for kp in person_keypoints:\n",
        "            ky, kx, kp_conf = kp[:3]  # Only take the first three values for each keypoint\n",
        "            if kp_conf > confidence_threshold:\n",
        "                cv2.circle(frame, (int(kx * x), int(ky * y)), 6, (0, 255, 0), -1)\n",
        "\n",
        "    draw_connections(frame, keypoints, edges, confidence_threshold)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "09b1EFuvkkyd"
      },
      "outputs": [],
      "source": [
        "# Function to draw keypoints on the frame\n",
        "def draw_keypoints(frame, keypoints, confidence_threshold):\n",
        "    y, x, c = frame.shape\n",
        "    shaped = np.multiply(keypoints.reshape((-1, 3)), [y, x, 1])\n",
        "\n",
        "    for kp in shaped:\n",
        "        ky, kx, kp_conf = kp[:3]\n",
        "        if kp_conf > confidence_threshold:\n",
        "            cv2.circle(frame, (int(kx), int(ky)), 6, (0, 255, 0), -1)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "xEBh1TyiknqM"
      },
      "outputs": [],
      "source": [
        "EDGES = {\n",
        "    (0, 1): (255, 0, 0),  # Red\n",
        "    (0, 2): (0, 255, 255),  # Cyan\n",
        "    (1, 3): (255, 0, 0),  # Red\n",
        "    (2, 4): (0, 255, 255),  # Cyan\n",
        "    (0, 5): (255, 0, 0),  # Red\n",
        "    (0, 6): (0, 255, 255),  # Cyan\n",
        "    (5, 7): (255, 0, 0),  # Red\n",
        "    (7, 9): (255, 0, 0),  # Red\n",
        "    (6, 8): (0, 255, 255),  # Cyan\n",
        "    (8, 10): (0, 255, 255),  # Cyan\n",
        "    (5, 6): (255, 255, 0),  # Yellow\n",
        "    (5, 11): (255, 0, 0),  # Red\n",
        "    (6, 12): (0, 255, 255),  # Cyan\n",
        "    (11, 12): (255, 255, 0),  # Yellow\n",
        "    (11, 13): (255, 0, 0),  # Red\n",
        "    (13, 15): (255, 0, 0),  # Red\n",
        "    (12, 14): (0, 255, 255),  # Cyan\n",
        "    (14, 16): (0, 255, 255)  # Cyan\n",
        "}\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4ajmji43kqas"
      },
      "outputs": [],
      "source": [
        "def draw_keypoints(frame, keypoints, confidence_threshold):\n",
        "    y, x, c = frame.shape\n",
        "\n",
        "    # Assuming the structure of each person's keypoints is [x1, y1, s1, x2, y2, s2, ..., x17, y17, s17]\n",
        "    for person_keypoints in keypoints[0]:\n",
        "        person_keypoints = person_keypoints.reshape((17, 3))\n",
        "\n",
        "        for i in range(0, 56, 3):\n",
        "            kx, ky, kp_conf = person_keypoints[:, i:i+3].T\n",
        "\n",
        "            valid_keypoints = kp_conf > confidence_threshold\n",
        "            valid_keypoints_indices = np.where(valid_keypoints)[0]\n",
        "\n",
        "            for idx in valid_keypoints_indices:\n",
        "                cv2.circle(frame, (int(kx[idx] * x), int(ky[idx] * y)), 6, (0, 255, 0), -1)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0Nay5R0PktQj"
      },
      "outputs": [],
      "source": [
        "def draw_connections(frame, keypoints, edges, confidence_threshold):\n",
        "    \"\"\"\n",
        "    Draw connections between keypoints on the frame.\n",
        "\n",
        "    Args:\n",
        "    - frame: The input frame.\n",
        "    - keypoints: Detected keypoints with their confidence scores.\n",
        "    - edges: A dictionary defining the connections between keypoints.\n",
        "    - confidence_threshold: Minimum confidence score to consider for drawing connections.\n",
        "    \"\"\"\n",
        "    h, w, _ = frame.shape\n",
        "\n",
        "    for edge, color in edges.items():\n",
        "        p1, p2 = edge\n",
        "        y1, x1, c1 = keypoints[0, p1]\n",
        "        y2, x2, c2 = keypoints[0, p2]\n",
        "\n",
        "        if c1 > confidence_threshold and c2 > confidence_threshold:\n",
        "            # Convert color values to integers\n",
        "            color = (int(color[0]), int(color[1]), int(color[2]))\n",
        "\n",
        "            cv2.line(frame, (int(x1 * w), int(y1 * h)), (int(x2 * w), int(y2 * h)), color, 2)\n",
        "            cv2.circle(frame, (int(x1 * w), int(y1 * h)), 5, color, -1)\n",
        "            cv2.circle(frame, (int(x2 * w), int(y2 * h)), 5, color, -1)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "cJSA0aX704Uz"
      },
      "outputs": [],
      "source": [
        "# Function to save highlight video using ffmpeg\n",
        "def save_highlight_video(frames, output_path):\n",
        "    # Write frames to temporary directory\n",
        "    temp_dir = '/content/temp_frames/'\n",
        "    if not os.path.exists(temp_dir):\n",
        "        os.makedirs(temp_dir)\n",
        "    for i, frame in enumerate(frames):\n",
        "        cv2.imwrite(os.path.join(temp_dir, f\"frame_{i}.png\"), frame.copy())\n",
        "\n",
        "    # Use ffmpeg to create video\n",
        "    cmd = f\"ffmpeg -framerate 25 -i {temp_dir}/frame_%d.png -c:v libx264 -r 30 -pix_fmt yuv420p {output_path}\"\n",
        "    subprocess.call(cmd, shell=True)\n",
        "\n",
        "    # Clean up temporary frames\n",
        "    shutil.rmtree(temp_dir)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "L5-w7n2cVE0C"
      },
      "outputs": [],
      "source": [
        "# Placeholder for frames\n",
        "frames = []\n",
        "\n",
        "# Counter for frames where the hand is raised\n",
        "hand_raised_frames = 0\n",
        "\n",
        "# Number of frames to consider for hand raising (adjust as needed)\n",
        "hand_raise_threshold = 2\n",
        "\n",
        "# Initialize VideoCapture\n",
        "cap = cv2.VideoCapture('/content/sample_data/IMG_8477.MP4')\n",
        "\n",
        "# Initialize an empty title\n",
        "current_title = 'NO SENAL'\n",
        "\n",
        "# Boolean variable to track if the hand is currently raised\n",
        "hand_raised = False\n",
        "\n",
        "# Initialize counters for frames when nose is higher than wrist\n",
        "nose_higher_frames = 0\n",
        "\n",
        "# Variable to keep track of frames processed\n",
        "frame_count = 0\n",
        "\n",
        "# Buffer to store the last 10 frames\n",
        "frame_buffer = deque(maxlen=10)\n",
        "\n",
        "# Initialize the model\n",
        "model = hub.load('https://tfhub.dev/google/movenet/multipose/lightning/1')\n",
        "movenet = model.signatures['serving_default']\n",
        "\n",
        "while cap.isOpened():\n",
        "    ret, frame = cap.read()\n",
        "\n",
        "    if not ret:\n",
        "        break  # Break the loop if no frame is read\n",
        "\n",
        "    # Append frame to the buffer\n",
        "    frame_buffer.append(frame.copy())\n",
        "\n",
        "    # Process every 8rd frame\n",
        "    if frame_count % 8 == 0:\n",
        "        # Resize image with the requested size\n",
        "        img = frame.copy()\n",
        "        img = tf.image.resize_with_pad(tf.expand_dims(img, axis=0), 384, 640)\n",
        "        input_img = tf.cast(img, dtype=tf.int32)\n",
        "\n",
        "        # Detection section\n",
        "        results = movenet(input_img)\n",
        "\n",
        "        # Get the raw keypoints_with_scores array\n",
        "        keypoints_with_scores_raw = results['output_0'].numpy()[:, :, :51].reshape((6, 17, 3))\n",
        "\n",
        "        # Render keypoints\n",
        "        loop_through_people(frame, keypoints_with_scores_raw, EDGES, 0.37)\n",
        "\n",
        "        # Logic to track hand raising\n",
        "        nose_y = keypoints_with_scores_raw[0,0,0]  # Extract y-coordinate of the nose (node 0)\n",
        "        wrist_y = keypoints_with_scores_raw[0,10,0]  # Extract y-coordinate of the right wrist (node 10)\n",
        "\n",
        "        if nose_y > wrist_y:\n",
        "            nose_higher_frames += 1\n",
        "            if nose_higher_frames > hand_raise_threshold and not hand_raised:\n",
        "                current_title = 'SENAL'\n",
        "                hand_raised = True\n",
        "\n",
        "                # When hand is raised, save the last 10 frames as a highlight video\n",
        "                highlight_video_path = f\"/content/sample_data/highlight_video_{frame_count}.mp4\"\n",
        "                save_highlight_video(list(frame_buffer), highlight_video_path)\n",
        "\n",
        "\n",
        "        else:\n",
        "            nose_higher_frames = max(0, nose_higher_frames - 1)\n",
        "            if hand_raised and nose_higher_frames == 0:\n",
        "                # Set the title to 'Movenet Multipose' when the hand goes back down\n",
        "                current_title = 'NO SENAL'\n",
        "                hand_raised = False\n",
        "\n",
        "        # Display the current title\n",
        "        cv2.putText(frame, current_title, (10, 30), cv2.FONT_HERSHEY_SIMPLEX, 1, (0, 0, 255), 2)\n",
        "\n",
        "        # Append frame to the list\n",
        "        frames.append(frame.copy())\n",
        "\n",
        "    frame_count += 1\n",
        "\n",
        "    if cv2.waitKey(10) & 0xFF == ord('q'):\n",
        "        break\n",
        "\n",
        "cap.release()\n",
        "cv2.destroyAllWindows()\n",
        "\n",
        "# Remove old frames (if any) before generating new ones\n",
        "frame_files = [f for f in os.listdir('/content') if f.startswith('frame_')]\n",
        "for frame_file in frame_files:\n",
        "    os.remove(os.path.join('/content', frame_file))\n",
        "\n",
        "# Save frames as images\n",
        "for i, frame in enumerate(frames):\n",
        "    cv2.imwrite(f\"frame_{i}.png\", frame.copy())\n",
        "\n",
        "# Convert images to video\n",
        "output_video_path = \"/content/sample_data/output_video.mp4\"\n",
        "cmd = f\"ffmpeg -framerate 25 -i frame_%d.png -c:v libx264 -r 30 -pix_fmt yuv420p {output_video_path}\"\n",
        "!{cmd}"
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "UaQ_inw50Kai"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}